# Open Deep Research - Azure Functions

Open Deep Researchã‚’ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã®Azure Functionså®Ÿè£…ã§ã™ã€‚

## æ¦‚è¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€LangChainã®[Open Deep Research](https://github.com/langchain-ai/open_deep_research)ã‚’Azure Functionsã§å‹•ä½œã•ã›ã€HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’è¡Œãˆã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚äººã®ä»‹å…¥ãªã—ã§è‡ªå‹•çš„ã«ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

### ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯[langchain-ai/open_deep_research](https://github.com/langchain-ai/open_deep_research/tree/main/src/open_deep_research)ã‚’å…ƒã«ã—ã¦ã€ä»¥ä¸‹ã®æ‹¡å¼µæ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™ï¼š

1. **Azure Functionsçµ±åˆ** - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’HTTPãƒˆãƒªã‚¬ãƒ¼ã§å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«å®Ÿè£…
2. **ãƒãƒãƒ¼ã‚¸ãƒ‰IDèªè¨¼** - Azure OpenAIã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«ãƒãƒãƒ¼ã‚¸ãƒ‰IDã‚’ä½¿ç”¨ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ 
3. **Bing Search APIå¯¾å¿œ** - Azureã®æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã§ã‚ã‚‹Bing Search API (v7)ã‚’ä½¿ã£ãŸæ¤œç´¢æ©Ÿèƒ½ã‚’è¿½åŠ 
4. **è‡ªå‹•å®Ÿè¡Œãƒ•ãƒ­ãƒ¼** - äººé–“ã®ä»‹å…¥ãªã—ã§å®Œå…¨è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’å®Ÿç¾

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ–¹æ³•

### å‰ææ¡ä»¶

- Azure Functionsã®é–‹ç™ºç’°å¢ƒ
- Python 3.9+
- Azure CLI

### ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’`local.settings.json`ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰ã¾ãŸã¯Azure Functionsã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆæœ¬ç•ªç’°å¢ƒç”¨ï¼‰ã«è¨­å®šã—ã¾ã™ï¼š

```json
{
  "IsEncrypted": false,
  "Values": {
    "FUNCTIONS_WORKER_RUNTIME": "python",
    "AzureWebJobsStorage": "UseDevelopmentStorage=true",
    
    "TAVILY_API_KEY": "<your_tavily_api_key>",
    "ANTHROPIC_API_KEY": "<your_anthropic_api_key>",
    "OPENAI_API_KEY": "<your_openai_api_key>",
    "PERPLEXITY_API_KEY": "<your_perplexity_api_key>",
    "EXA_API_KEY": "<your_exa_api_key>",
    "PUBMED_API_KEY": "<your_pubmed_api_key>",
    "PUBMED_EMAIL": "<your_email@example.com>",
    "LINKUP_API_KEY": "<your_linkup_api_key>",
    "GOOGLE_API_KEY": "<your_google_api_key>",
    "GOOGLE_CX": "<your_google_custom_search_engine_id>",
    "BING_API_KEY": "<your_bing_api_key>",
    "BING_ENDPOINT": "https://api.bing.microsoft.com/v7.0/search"
  }
}
```

ä½¿ç”¨ã™ã‚‹æ¤œç´¢APIã¨ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦å¿…è¦ãªAPIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚

### Azure OpenAIã®ãƒãƒãƒ¼ã‚¸ãƒ‰IDèªè¨¼

ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€Azure OpenAIã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«ãƒãƒãƒ¼ã‚¸ãƒ‰IDã‚’ä½¿ç”¨ã™ã‚‹æ©Ÿèƒ½ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ãƒãƒãƒ¼ã‚¸ãƒ‰IDã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«ä¿å­˜ã›ãšã«å®‰å…¨ã«Azure OpenAIã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚

#### è¨­å®šæ‰‹é †

1. **ãƒãƒãƒ¼ã‚¸ãƒ‰IDã®æœ‰åŠ¹åŒ–**:
   Azure Portalã§é–¢æ•°ã‚¢ãƒ—ãƒªã®IDã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ã‚·ã‚¹ãƒ†ãƒ å‰²ã‚Šå½“ã¦ãƒãƒãƒ¼ã‚¸ãƒ‰IDã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚

2. **ã‚¢ã‚¯ã‚»ã‚¹æ¨©ã®ä»˜ä¸**:
   Azure OpenAIãƒªã‚½ãƒ¼ã‚¹ã®IAMã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã€é–¢æ•°ã‚¢ãƒ—ãƒªã®ãƒãƒãƒ¼ã‚¸ãƒ‰IDã«ã€ŒCognitive Services OpenAI Userã€ã¾ãŸã¯ã€ŒCognitive Services OpenAI Contributorã€ã®ãƒ­ãƒ¼ãƒ«ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚

3. **å¿…è¦ãªç’°å¢ƒå¤‰æ•°**:
   ãƒãƒãƒ¼ã‚¸ãƒ‰IDèªè¨¼ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã®ã¿ãŒå¿…è¦ã§ã™ï¼š
   ```json
   {
     "AZURE_OPENAI_ENDPOINT": "https://<your-resource-name>.openai.azure.com/"
   }
   ```

4. **ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŒ‡å®š**:
   ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã« `planner_provider` ã¾ãŸã¯ `writer_provider` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ `"azure-openai"` ã«è¨­å®šã—ã¾ã™ï¼š
   ```json
   {
     "topic": "AIã®æœ€æ–°å‹•å‘",
     "planner_provider": "azure-openai",
     "planner_model": "<your-deployment-name>"
   }
   ```

### Bing Search APIã®ä½¿ç”¨

ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€Webæ¤œç´¢ã«Bing Search API (v7)ã‚’ä½¿ç”¨ã™ã‚‹æ©Ÿèƒ½ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚Bing Search APIã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€Microsoftã®é«˜å“è³ªãªæ¤œç´¢çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«æ´»ç”¨ã§ãã¾ã™ã€‚

#### è¨­å®šæ‰‹é †

1. **Bing Search APIã‚­ãƒ¼ã®å–å¾—**:
   [Azure Portal](https://portal.azure.com)ã§Bing Search v7ãƒªã‚½ãƒ¼ã‚¹ã‚’ä½œæˆã—ã€APIã‚­ãƒ¼ã‚’å–å¾—ã—ã¾ã™ã€‚

2. **ç’°å¢ƒå¤‰æ•°ã®è¨­å®š**:
   ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™ï¼š
   ```json
   {
     "BING_API_KEY": "<your-bing-api-key>",
     "BING_ENDPOINT": "https://api.bing.microsoft.com/v7.0/search"
   }
   ```

3. **æ¤œç´¢APIã¨ã—ã¦æŒ‡å®š**:
   ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã« `search_api` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ `"bing"` ã«è¨­å®šã—ã¾ã™ï¼š
   ```json
   {
     "topic": "AIã®æœ€æ–°å‹•å‘",
     "search_api": "bing",
     "search_api_config": {
       "num_results": 8,
       "freshness": "Month"
     }
   }
   ```

4. **è¿½åŠ ã®è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³**:
   - `num_results`: å„ã‚¯ã‚¨ãƒªã‚ãŸã‚Šã®æ¤œç´¢çµæœæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
   - `freshness`: çµæœã®é®®åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆ"Day"ã€"Week"ã€"Month"ï¼‰

### ãƒ‡ãƒ—ãƒ­ã‚¤

#### ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ

```bash
func start
```

#### Azure ã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤

```bash
az login
az functionapp create --resource-group <resource-group-name> --consumption-plan-location <location> --runtime python --runtime-version 3.9 --functions-version 4 --name <app-name> --storage-account <storage-account-name>
func azure functionapp publish <app-name>
```

## ä½¿ç”¨æ–¹æ³•

### ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹

```bash
curl -X POST https://<app-name>.azurewebsites.net/api/generate-report \
  -H "Content-Type: application/json" \
  -H "x-functions-key: <function-key>" \
  -d '{
    "topic": "AIã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æœ€è¿‘ã®é€²æ­©",
    "search_api": "bing",
    "planner_provider": "azure-openai",
    "planner_model": "gpt-4",
    "writer_provider": "azure-openai",
    "writer_model": "gpt-35-turbo",
    "max_search_depth": 2,
    "number_of_queries": 3,
    "search_api_config": {
      "num_results": 8,
      "freshness": "Month"
    }
  }'
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

```json
{
  "report": "## å°å…¥\n\nAIã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¯...",
  "status": "success"
}
```

## è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | èª¬æ˜ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ |
|------------|------|------------|
| `topic` | ãƒ¬ãƒãƒ¼ãƒˆã®ãƒˆãƒ”ãƒƒã‚¯ï¼ˆå¿…é ˆï¼‰ | - |
| `search_api` | æ¤œç´¢APIï¼ˆ"tavily", "perplexity", "exa", "bing"ãªã©ï¼‰ | "tavily" |
| `planner_provider` | ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | "anthropic" |
| `planner_model` | ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ãƒ¢ãƒ‡ãƒ«å | "claude-3-7-sonnet-latest" |
| `writer_provider` | ãƒ©ã‚¤ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | "anthropic" |
| `writer_model` | ãƒ©ã‚¤ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«å | "claude-3-5-sonnet-latest" |
| `max_search_depth` | æ¤œç´¢ã®æœ€å¤§æ·±åº¦ | 1 |
| `number_of_queries` | ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã®æ¤œç´¢ã‚¯ã‚¨ãƒªæ•° | 2 |
| `report_structure` | ãƒ¬ãƒãƒ¼ãƒˆæ§‹é€ ã®æŒ‡å®š | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ§‹é€  |
| `search_api_config` | æ¤œç´¢APIå›ºæœ‰ã®è¨­å®š | {} |

### search_api_configã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³

å„æ¤œç´¢APIã¯ç‰¹å®šã®è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼š

* **Bing**
  - `num_results`: å„ã‚¯ã‚¨ãƒªã®çµæœæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
  - `freshness`: çµæœã®é®®åº¦ï¼ˆ"Day", "Week", "Month"ï¼‰

* **Exa**
  - `max_characters`: æœ€å¤§æ–‡å­—æ•°
  - `num_results`: çµæœæ•°
  - `include_domains`: å«ã‚ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆãƒªã‚¹ãƒˆï¼‰
  - `exclude_domains`: é™¤å¤–ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆãƒªã‚¹ãƒˆï¼‰
  - `subpages`: ã‚µãƒ–ãƒšãƒ¼ã‚¸ã®æ•°

* **ArXiv**
  - `load_max_docs`: æœ€å¤§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
  - `get_full_documents`: å®Œå…¨ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹ã‹ã©ã†ã‹
  - `load_all_available_meta`: ã™ã¹ã¦ã®åˆ©ç”¨å¯èƒ½ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ã‹ã©ã†ã‹

* **PubMed**
  - `top_k_results`: ä¸Šä½kä»¶ã®çµæœ
  - `email`: é›»å­ãƒ¡ãƒ¼ãƒ«
  - `api_key`: APIã‚­ãƒ¼
  - `doc_content_chars_max`: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ€å¤§æ–‡å­—æ•°

* **Linkup**
  - `depth`: æ¤œç´¢ã®æ·±ã•

## æ³¨æ„äº‹é …

- ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚Azure Functionsã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
- APIã‚­ãƒ¼ã®ä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆã‚’ç›£è¦–ã—ã¦ãã ã•ã„ã€‚
- å¤§é‡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹å ´åˆã¯ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
- ãƒãƒãƒ¼ã‚¸ãƒ‰IDã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€é–¢æ•°ã‚¢ãƒ—ãƒªã«é©åˆ‡ãªã‚¢ã‚¯ã‚»ã‚¹æ¨©ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

# Open Deep Research
 
Open Deep Research is an open source assistant that automates research and produces customizable reports on any topic. It allows you to customize the research and writing process with specific models, prompts, report structure, and search tools. 

![report-generation](https://github.com/user-attachments/assets/6595d5cd-c981-43ec-8e8b-209e4fefc596)

## ğŸš€ Quickstart

Ensure you have API keys set for your desired search tools and models.

Available search tools:

* [Tavily API](https://tavily.com/) - General web search
* [Perplexity API](https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api) - General web search
* [Exa API](https://exa.ai/) - Powerful neural search for web content
* [ArXiv](https://arxiv.org/) - Academic papers in physics, mathematics, computer science, and more
* [PubMed](https://pubmed.ncbi.nlm.nih.gov/) - Biomedical literature from MEDLINE, life science journals, and online books
* [Linkup API](https://www.linkup.so/) - General web search
* [DuckDuckGo API](https://duckduckgo.com/) - General web search
* [Google Search API/Scrapper](https://google.com/) - Create custom search engine [here](https://programmablesearchengine.google.com/controlpanel/all) and get API key [here](https://developers.google.com/custom-search/v1/introduction)

Open Deep Research uses a planner LLM for report planning and a writer LLM for report writing: 

* You can select any model that is integrated [with the `init_chat_model()` API](https://python.langchain.com/docs/how_to/chat_models_universal_init/)
* See full list of supported integrations [here](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)

### Using the package

```bash
pip install open-deep-research
```

As mentioned above, ensure API keys for LLMs and search tools are set: 
```bash
export TAVILY_API_KEY=<your_tavily_api_key>
export ANTHROPIC_API_KEY=<your_anthropic_api_key>
```

See [src/open_deep_research/graph.ipynb](src/open_deep_research/graph.ipynb) for example usage in a Jupyter notebook:

Compile the graph:
```python
from langgraph.checkpoint.memory import MemorySaver
from open_deep_research.graph import builder
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)
```

Run the graph with a desired topic and configuration:
```python
import uuid 
thread = {"configurable": {"thread_id": str(uuid.uuid4()),
                           "search_api": "tavily",
                           "planner_provider": "anthropic",
                           "planner_model": "claude-3-7-sonnet-latest",
                           "writer_provider": "anthropic",
                           "writer_model": "claude-3-5-sonnet-latest",
                           "max_search_depth": 1,
                           }}

topic = "Overview of the AI inference market with focus on Fireworks, Together.ai, Groq"
async for event in graph.astream({"topic":topic,}, thread, stream_mode="updates"):
    print(event)
```

The graph will stop when the report plan is generated, and you can pass feedback to update the report plan:
```python
from langgraph.types import Command
async for event in graph.astream(Command(resume="Include a revenue estimate (ARR) in the sections"), thread, stream_mode="updates"):
    print(event)
```

When you are satisfied with the report plan, you can pass `True` to proceed to report generation:
```python
async for event in graph.astream(Command(resume=True), thread, stream_mode="updates"):
    print(event)
```

### Running LangGraph Studio UI locally

Clone the repository:
```bash
git clone https://github.com/langchain-ai/open_deep_research.git
cd open_deep_research
```

Then edit the `.env` file to customize the environment variables according to your needs. These environment variables control the model selection, search tools, and other configuration settings. When you run the application, these values will be automatically loaded via `python-dotenv` (because `langgraph.json` point to the "env" file).
```bash
cp .env.example .env
```

Set whatever APIs needed for your model and search tools.

Here are examples for several of the model and tool integrations available:
```bash
export TAVILY_API_KEY=<your_tavily_api_key>
export ANTHROPIC_API_KEY=<your_anthropic_api_key>
export OPENAI_API_KEY=<your_openai_api_key>
export PERPLEXITY_API_KEY=<your_perplexity_api_key>
export EXA_API_KEY=<your_exa_api_key>
export PUBMED_API_KEY=<your_pubmed_api_key>
export PUBMED_EMAIL=<your_email@example.com>
export LINKUP_API_KEY=<your_linkup_api_key>
export GOOGLE_API_KEY=<your_google_api_key>
export GOOGLE_CX=<your_google_custom_search_engine_id>
```

Launch the assistant with the LangGraph server locally, which will open in your browser:

#### Mac

```bash
# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies and start the LangGraph server
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev
```

#### Windows / Linux

```powershell
# Install dependencies 
pip install -e .
pip install -U "langgraph-cli[inmem]" 

# Start the LangGraph server
langgraph dev
```

Use this to open the Studio UI:
```
- ğŸš€ API: http://127.0.0.1:2024
- ğŸ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- ğŸ“š API Docs: http://127.0.0.1:2024/docs
```

(1) Provide a `Topic` and hit `Submit`:

<img width="1326" alt="input" src="https://github.com/user-attachments/assets/de264b1b-8ea5-4090-8e72-e1ef1230262f" />

(2) This will generate a report plan and present it to the user for review.

(3) We can pass a string (`"..."`) with feedback to regenerate the plan based on the feedback.

<img width="1326" alt="feedback" src="https://github.com/user-attachments/assets/c308e888-4642-4c74-bc78-76576a2da919" />

(4) Or, we can just pass `true` to accept the plan.

<img width="1480" alt="accept" src="https://github.com/user-attachments/assets/ddeeb33b-fdce-494f-af8b-bd2acc1cef06" />

(5) Once accepted, the report sections will be generated.

<img width="1326" alt="report_gen" src="https://github.com/user-attachments/assets/74ff01cc-e7ed-47b8-bd0c-4ef615253c46" />

The report is produced as markdown.

<img width="1326" alt="report" src="https://github.com/user-attachments/assets/92d9f7b7-3aea-4025-be99-7fb0d4b47289" />

## ğŸ“– Customizing the report

You can customize the research assistant's behavior through several parameters:

- `report_structure`: Define a custom structure for your report (defaults to a standard research report format)
- `number_of_queries`: Number of search queries to generate per section (default: 2)
- `max_search_depth`: Maximum number of reflection and search iterations (default: 2)
- `planner_provider`: Model provider for planning phase (default: "anthropic", but can be any provider from supported integrations with `init_chat_model` as listed [here](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html))
- `planner_model`: Specific model for planning (default: "claude-3-7-sonnet-latest")
- `writer_provider`: Model provider for writing phase (default: "anthropic", but can be any provider from supported integrations with `init_chat_model` as listed [here](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html))
- `writer_model`: Model for writing the report (default: "claude-3-5-sonnet-latest")
- `search_api`: API to use for web searches (default: "tavily", options include "perplexity", "exa", "arxiv", "pubmed", "linkup")

These configurations allow you to fine-tune the research process based on your needs, from adjusting the depth of research to selecting specific AI models for different phases of report generation.

### Search API Configuration

Not all search APIs support additional configuration parameters. Here are the ones that do:

- **Exa**: `max_characters`, `num_results`, `include_domains`, `exclude_domains`, `subpages`
  - Note: `include_domains` and `exclude_domains` cannot be used together
  - Particularly useful when you need to narrow your research to specific trusted sources, ensure information accuracy, or when your research requires using specified domains (e.g., academic journals, government sites)
  - Provides AI-generated summaries tailored to your specific query, making it easier to extract relevant information from search results
- **ArXiv**: `load_max_docs`, `get_full_documents`, `load_all_available_meta`
- **PubMed**: `top_k_results`, `email`, `api_key`, `doc_content_chars_max`
- **Linkup**: `depth`

Example with Exa configuration:
```python
thread = {"configurable": {"thread_id": str(uuid.uuid4()),
                           "search_api": "exa",
                           "search_api_config": {
                               "num_results": 5,
                               "include_domains": ["nature.com", "sciencedirect.com"]
                           },
                           # Other configuration...
                           }}
```

### Model Considerations

(1) You can pass any planner and writer models that are integrated [with the `init_chat_model()` API](https://python.langchain.com/docs/how_to/chat_models_universal_init/). See full list of supported integrations [here](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html).

(2) **The planner and writer models need to support structured outputs**: Check whether structured outputs are supported by the model you are using [here](https://python.langchain.com/docs/integrations/chat/).

(3) With Groq, there are token per minute (TPM) limits if you are on the `on_demand` service tier:
- The `on_demand` service tier has a limit of `6000 TPM`
- You will want a [paid plan](https://github.com/cline/cline/issues/47#issuecomment-2640992272) for section writing with Groq models

(4) `deepseek-R1` [is not strong at function calling](https://api-docs.deepseek.com/guides/reasoning_model), which the assistant uses to generate structured outputs for report sections and report section grading. See example traces [here](https://smith.langchain.com/public/07d53997-4a6d-4ea8-9a1f-064a85cd6072/r).  
- Consider providers that are strong at function calling such as OpenAI, Anthropic, and certain OSS models like Groq's `llama-3.3-70b-versatile`.
- If you see the following error, it is likely due to the model not being able to produce structured outputs (see [trace](https://smith.langchain.com/public/8a6da065-3b8b-4a92-8df7-5468da336cbe/r)):
```
groq.APIError: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.
```

## How it works
   
1. `Plan and Execute` - Open Deep Research follows a [plan-and-execute workflow](https://github.com/assafelovic/gpt-researcher) that separates planning from research, allowing for human-in-the-loop approval of a report plan before the more time-consuming research phase. It uses, by default, a [reasoning model](https://www.youtube.com/watch?v=f0RbwrBcFmc) to plan the report sections. During this phase, it uses web search to gather general information about the report topic to help in planning the report sections. But, it also accepts a report structure from the user to help guide the report sections as well as human feedback on the report plan.
   
2. `Research and Write` - Each section of the report is written in parallel. The research assistant uses web search via [Tavily API](https://tavily.com/), [Perplexity](https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api), [Exa](https://exa.ai/), [ArXiv](https://arxiv.org/), [PubMed](https://pubmed.ncbi.nlm.nih.gov/) or [Linkup](https://www.linkup.so/) to gather information about each section topic. It will reflect on each report section and suggest follow-up questions for web search. This "depth" of research will proceed for any many iterations as the user wants. Any final sections, such as introductions and conclusions, are written after the main body of the report is written, which helps ensure that the report is cohesive and coherent. The planner determines main body versus final sections during the planning phase.

3. `Managing different types` - Open Deep Research is built on LangGraph, which has native support for configuration management [using assistants](https://langchain-ai.github.io/langgraph/concepts/assistants/). The report `structure` is a field in the graph configuration, which allows users to create different assistants for different types of reports. 

## UX

### Local deployment

Follow the [quickstart](#-quickstart) to start LangGraph server locally.

### Hosted deployment
 
You can easily deploy to [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/#deployment-options). 

---

## è¬è¾

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€[LangChain](https://github.com/langchain-ai)ãƒãƒ¼ãƒ ãŒé–‹ç™ºã—ãŸ[Open Deep Research](https://github.com/langchain-ai/open_deep_research)ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ã€‚ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚³ãƒ¼ãƒ‰ã¨ãã®ç´ æ™´ã‚‰ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æä¾›ã—ã¦ãã ã•ã£ãŸLangChainãƒãƒ¼ãƒ ã«æ·±ãæ„Ÿè¬ã„ãŸã—ã¾ã™ã€‚

### ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Open Deep Researchã¨åŒæ§˜ã«[MIT License](https://opensource.org/licenses/MIT)ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚
