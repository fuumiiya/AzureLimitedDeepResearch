import json
import logging
import asyncio
import azure.functions as func
from langgraph.checkpoint.memory import MemorySaver
from open_deep_research.configuration import Configuration
import uuid
import os
import sys
import inspect
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "src")))
sys.modules.pop("open_deep_research.graph", None)
sys.modules.pop("open_deep_research", None)  # â†ã“ã‚Œã‚‚é‡è¦

import traceback
from azure.identity import DefaultAzureCredential
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import QueryType, VectorizableTextQuery
from langchain_openai import AzureChatOpenAI
from functools import wraps
import aiohttp
import open_deep_research.utils as odr_utils
import langchain.chat_models as lcm
from typing import List, Dict, Optional, Any
import datetime
from azure.core.exceptions import HttpResponseError

# âœ… ãƒ­ã‚¬ãƒ¼è¨­å®šã‚’ãƒ«ãƒ¼ãƒˆã§å†æ§‹æˆï¼ˆforce=True ã§ä»–ã®è¨­å®šã‚’ãƒªã‚»ãƒƒãƒˆï¼‰
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
    force=True
)

# âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚’è¿½åŠ ã—ãŸã„å ´åˆã®ã¿ã“ã‚Œã‚’æ®‹ã™
file_handler = logging.FileHandler('function_app.log')
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(
    logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
)
logging.getLogger().addHandler(file_handler)
logger = logging.getLogger("app")


app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)
# =======================================================================
#  HTTP Trigger main
# =======================================================================
@app.function_name(name="HttpTrigger")
@app.route(route="main", methods=["POST"])
async def main(req: func.HttpRequest) -> func.HttpResponse:
    import importlib
    import open_deep_research.graph as graph_module
    importlib.reload(graph_module)
    graph = graph_module.graph

    print("ğŸ”¥ Using graph from:", graph_module.__file__)
    logger.info(f"ğŸ”¥ Using graph from: {graph_module.__file__}")
    logger.info(f"âœ… utils.py path: {inspect.getfile(odr_utils)}")

    """
    HTTP ãƒˆãƒªã‚¬ãƒ¼ã§ Open Deep Research ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
    """
    logger.info('Open Deep Research function triggered')

    try:
        # --- ãƒªã‚¯ã‚¨ã‚¹ãƒˆå–å¾— -------------------------------------------------
        req_body = req.get_json()
        logger.info(f"å—ä¿¡ãƒœãƒ‡ã‚£:\n{json.dumps(req_body, indent=2, ensure_ascii=False)}")

        if 'topic' not in req_body:
            return func.HttpResponse(
                json.dumps({"error": "'topic' ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å¿…é ˆã§ã™ã€‚"}),
                mimetype="application/json",
                status_code=400,
            )

        topic = req_body['topic']
        logger.info(f"ãƒˆãƒ”ãƒƒã‚¯: {topic}")

        # --- thread_config ä½œæˆ -------------------------------------------
        thread_config = {
            "thread_id": str(uuid.uuid4()),
            "search_api": req_body.get('search_api', "tavily"),
            "planner_provider": req_body.get('planner_provider', "anthropic"),
            "planner_model": req_body.get('planner_model', "claude-3-7-sonnet-latest"),
            "writer_provider": req_body.get('writer_provider', "anthropic"),
            "writer_model": req_body.get('writer_model', "claude-3-5-sonnet-latest"),
            "max_search_depth": req_body.get('max_search_depth', 2),
        }
        # optional
        for opt in ('report_structure', 'number_of_queries', 'search_api_config'):
            if opt in req_body:
                thread_config[opt] = req_body[opt]

        logger.info(f"thread_config:\n{json.dumps(thread_config, indent=2, ensure_ascii=False)}")
        thread = {"configurable": thread_config}

        # --- ã‚°ãƒ©ãƒ•å®Ÿè¡Œ -----------------------------------------------------
        #from open_deep_research.graph import graph   # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯ãƒ‘ãƒƒãƒå¾Œ
        logger.info("graph.ainvoke é–‹å§‹")
        result = await graph.ainvoke({"topic": topic}, thread)
        logger.info("[graph.ainvoke result - formatted]\n%s", json.dumps(result, ensure_ascii=False, indent=2))

        logger.info("graph.ainvoke å®Œäº†")

        final_report = result.get("final_report", "")
        logger.info("[Final Report - formatted output]\n\n%s", final_report)

        return func.HttpResponse(
            json.dumps({"report": final_report, "status": "success"}),
            mimetype="application/json",
        )

    except Exception as e:
        logger.error("ã‚¨ãƒ©ãƒ¼:", exc_info=True)
        return func.HttpResponse(
            json.dumps({"error": str(e), "status": "error"}),
            mimetype="application/json",
            status_code=500,
        )

# =======================================================================
#  å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =======================================================================
def get_azure_openai_token_authenticator():
    """
    Managed-Identity ã§ Azure OpenAI ã‚’å‘¼ã¶ãŸã‚ã®
    **åŒæœŸ** ãƒˆãƒ¼ã‚¯ãƒ³ãƒ—ãƒ­ãƒã‚¤ãƒ€ã€‚
    """
    from azure.identity import DefaultAzureCredential
    credential = DefaultAzureCredential()
    scope = "https://cognitiveservices.azure.com/.default"

    def token_provider() -> str:
        # DefaultAzureCredential.get_token ã¯åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ãªã®ã§
        # ãã®ã¾ã¾å‘¼ã‚“ã§æ–‡å­—åˆ—ã‚’è¿”ã›ã° OK
        return credential.get_token(scope).token

    return token_provider

async def run_async(func):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, func)

# ---------------- Azure AI Search ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ --------------------
async def azure_ai_search_async(
    search_service: str,
    search_key: str | None,
    index_name: str,
    query: str,
    top_k: int = 5,
    vector_fields: str | None  = None,
    semantic_configuration: str | None = None,
    use_managed_identity: bool = False,
) -> List[Dict[str, Any]]:
    logger.info(f"Azure AI Search: {query}")
    credential = DefaultAzureCredential() if use_managed_identity else AzureKeyCredential(search_key)
    client = SearchClient(
        endpoint=f"https://{search_service}.search.windows.net/",
        index_name=index_name,
        credential=credential,
        api_version="2023-10-01-Preview"
    )

    # --- æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’çµ„ã¿ç«‹ã¦ --------------------------
    search_kwargs: dict[str, Any] = {
        "search_text": query,
        "top": top_k,
        "select": ["metadata_spo_item_name",
                   "metadata_spo_item_path",
                   "textItems"],
    }

    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å…¥ã‚ŒãŸã„ã¨ã
    if vector_fields:
        search_kwargs["vector_queries"] = [
            {
                "kind": "text",
                "text": query,
                "k": top_k,
                "fields": vector_fields,
            }
        ]

    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’å…¥ã‚ŒãŸã„ã¨ã
    if semantic_configuration:
        search_kwargs.update(
            {
                "query_type": QueryType.SEMANTIC,
                "semantic_configuration_name": semantic_configuration,
                "query_answer": "extractive",
                "query_caption": "extractive",
            }
        )
    try:
        res = await run_async(lambda: client.search(**search_kwargs))

    except HttpResponseError as e:
        logger.error("Status: %s\nMessage: %s", e.status_code, e.message)
        if e.response:
            logger.error("Body: %s", e.response.text())   # â† ã“ã“ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦è²¼ã£ã¦ãã ã•ã„
        raise

    docs_raw = list(res)
    docs: List[Dict[str, Any]] = []
    for d in docs_raw:
        docs.append(
            {
                "title": d.get("metadata_spo_item_name", ""),
                "url":   d.get("metadata_spo_item_path", ""),
                "content": (
                    # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å„ªå…ˆã€ãªã‘ã‚Œã°æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆ 3 æœ¬
                    d["@search.captions"][0].text
                    if d.get("@search.captions")
                    else " ".join(d.get("textItems", [])[:3])
                ),
                "raw_content": " ".join(d.get("textItems", [])),   # å…¨æ–‡ï¼ˆé•·ãã¦ OKï¼‰
                "score": d.get("@search.score", 0)                  # ãªãã¦ã‚‚å‹•ããŒä¸€å¿œ
            }
        )
    logger.info("ğŸ” Azure AI Search: æ¤œç´¢ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•° = %d", len(docs))
    for i, doc in enumerate(docs[:3], 1):  # å¤šã™ãã‚‹ã¨ãƒ­ã‚°ãŒå¤§å¤‰ãªã®ã§3ä»¶ã ã‘
        logger.info("ğŸ“„ Doc %d: title=%s, url=%s, content(æŠœç²‹)=%s", i, doc["title"], doc["url"], doc["content"][:100])

    return docs


def format_search_results_to_string(docs: List[Dict[str, Any]]) -> str:
    parts = []
    for d in docs:
        head = f"ã‚¿ã‚¤ãƒˆãƒ«: {d.get('title')}\nURL: {d.get('url')}"
        body = []
        if "caption" in d:
            body.append(f"è¦ç´„: {d['caption']}")
        for t in d.get("text_items", [])[:3]:
            body.append(f"- {t}")
        parts.append(head + ("\n" + "\n".join(body) if body else ""))
    return "\n\n" + "=" * 50 + "\n\n".join(parts) + "\n\n" + "=" * 50

# =======================================================================
#  ã“ã“ã‹ã‚‰ãŒ **å¤‰æ›´ç‚¹** ã§ã™
# =======================================================================
# <<< CHANGED >>> --------------------------------------------------------
#   1. æ—§ã€Œå·¨å¤§ãƒ‘ãƒƒãƒã€ãƒ–ãƒ­ãƒƒã‚¯ã‚’ **å‰Šé™¤** ã—ã¾ã—ãŸ
#   2. æœ€å°é™ã® select_and_execute_search ãƒ‘ãƒƒãƒã‚’æ–°è¨­ã—ã¾ã—ãŸ
# -----------------------------------------------------------------------

# ã‚ªãƒªã‚¸ãƒŠãƒ«é–¢æ•°ã‚’ä¿æŒ
_orig_search = odr_utils.select_and_execute_search

async def _patched_select_and_execute_search(search_api, query_list, params=None):
    if search_api == "azure_ai_search":
        cfg = params or {}
        service    = os.environ["AZURE_SEARCH_SERVICE"]
        api_key    = os.environ.get("AZURE_SEARCH_KEY")
        use_mi     = api_key is None
        index_name = cfg.get("index_name", os.getenv("AZURE_SEARCH_INDEX","default"))
        top_k      = cfg.get("top_k", 5)
        vec_fields = cfg.get("vector_fields","embedding")
        sem_conf   = cfg.get("semantic_configuration")


        # 1) å…¨ã‚¯ã‚¨ãƒªåˆ†ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¾æ›¸ã‚’ã¾ã¨ã‚ã‚‹
        all_responses: List[Dict[str,Any]] = []        # â† ãƒªã‚¹ãƒˆåã‚’ã‚ã‹ã‚Šæ˜“ã
        for q in query_list:
            docs = await azure_ai_search_async(
                service, api_key, index_name,
                q, top_k, vec_fields, sem_conf, use_mi
            )
            # â˜…ã“ã“ã§ â€œ1ã‚¯ã‚¨ãƒªï¼1è¾æ›¸â€ ã«ã¾ã¨ã‚ã‚‹
            all_responses.append({
                "query": q,
                "results": docs,            # â† ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãŒæœŸå¾…ã™ã‚‹ã‚­ãƒ¼
                "follow_up_questions": None,
                "answer": None,
                "images": [],
            })

        # 2) ã¾ã¨ã‚ãŸè¾æ›¸ãƒªã‚¹ãƒˆã‚’ä¸€åº¦ã ã‘ dedupe + ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        return odr_utils.deduplicate_and_format_sources(
            all_responses,
            max_tokens_per_source=4000,
            include_raw_content=False,
        )

    # ãã‚Œä»¥å¤–ã¯å…ƒã®å‡¦ç†
    return await _orig_search(search_api, query_list, params or {})

# å·®ã—æ›¿ãˆ
odr_utils.select_and_execute_search = _patched_select_and_execute_search

# init_chat_modelã®ãƒ‘ãƒƒãƒå‡¦ç†
original_init_chat_model = lcm.init_chat_model

@wraps(original_init_chat_model)
def patched_init_chat_model(model, model_provider=None, **kwargs):
    """
    Azure OpenAIã‚’ãƒãƒãƒ¼ã‚¸ãƒ‰IDã§å‘¼ã³å‡ºã™ãŸã‚ã«ãƒ‘ãƒƒãƒã•ã‚ŒãŸinit_chat_modelé–¢æ•°
    """
    logger.info(f"init_chat_model called with model={model}, model_provider={model_provider}, kwargs={kwargs}")

    # Azure OpenAIã®å ´åˆã¯ãƒãƒãƒ¼ã‚¸ãƒ‰IDã‚’ä½¿ç”¨
    if model_provider == "azure-openai":
        logger.info("Azure OpenAIåˆ†å²ã«å…¥ã‚Šã¾ã—ãŸ")

        # å¿…è¦ãªç’°å¢ƒå¤‰æ•°ã‚’å–å¾—
        endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
        if not endpoint:
            raise ValueError("AZURE_OPENAI_ENDPOINT ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # APIãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        api_version = os.environ.get("AZURE_OPENAI_API_VERSION")
        if not api_version:
            raise ValueError("AZURE_OPENAI_API_VERSION ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # ç’°å¢ƒå¤‰æ•°ã®å€¤ã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"ç’°å¢ƒå¤‰æ•°: AZURE_OPENAI_ENDPOINT={endpoint}, "
                   f"\nAZURE_OPENAI_API_VERSION={api_version}, "
                   f"\nOPENAI_API_VERSION={os.environ.get('OPENAI_API_VERSION')}")

        # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        deployment_name = os.environ.get("AZURE_OPENAI_DEPLOYMENT", model)
        token_provider = get_azure_openai_token_authenticator()

        # è¿½åŠ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        azure_kwargs = {
            "azure_endpoint": endpoint,
            "azure_deployment": deployment_name,
            "azure_ad_token_provider": token_provider
        }
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã‚’çµåˆ
        model_kwargs = kwargs.pop("model_kwargs", {})
        combined_kwargs = {**azure_kwargs, **model_kwargs, **kwargs}

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
        if 'messages' in combined_kwargs:
            logger.info("ã€Azure OpenAIãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ç›´å‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘:")
            for i, msg in enumerate(combined_kwargs['messages'], 1):
                logger.info(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {i}:")
                logger.info(json.dumps(msg, indent=2, ensure_ascii=False))

        # å®Œå…¨ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆæ§‹é€ ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        logger.info("ã€Azure OpenAIãƒªã‚¯ã‚¨ã‚¹ãƒˆæ§‹é€ ã€‘:")
        logger.info(f"URL: {endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}")
        logger.info("Method: POST")
        logger.info("Headers: Content-Type: application/json, Authorization: Bearer <token>")

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®æ§‹é€ 
        request_body = {
            "messages": combined_kwargs.get('messages', []),
            "model": model,
            "temperature": combined_kwargs.get('temperature', 0.7),
            "max_tokens": combined_kwargs.get('max_tokens', 4000),
            "top_p": combined_kwargs.get('top_p', 1.0),
            "frequency_penalty": combined_kwargs.get('frequency_penalty', 0),
            "presence_penalty": combined_kwargs.get('presence_penalty', 0)
        }
        logger.info(f"Request Body: {json.dumps(request_body, indent=2, ensure_ascii=False)}")

        # AzureChatOpenAIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        try:
            # combined_kwargsã®ä¸­èº«ã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info("ã€AzureChatOpenAIã«æ¸¡ã™combined_kwargsã®ä¸­èº«ã€‘:")
            logger.info(json.dumps(combined_kwargs, indent=2, ensure_ascii=False, default=str))

            client = AzureChatOpenAI(**combined_kwargs)
            logger.info("AzureChatOpenAIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆã«æˆåŠŸã—ã¾ã—ãŸ")

            try:
                from open_deep_research.state import Sections
                structured = client.with_structured_output(Sections)
                logger.info("structured_llm.output_schema: %s", structured.output_schema.schema())
            except Exception as e:
                logger.warning("ã‚¹ã‚­ãƒ¼ãƒç¢ºèªå¤±æ•—: %s", str(e))

            logger.info(f"client.model: {getattr(client, 'model', 'è¨­å®šãªã—')}")
            logger.info(f"client.model_provider: {getattr(client, 'model_provider', 'è¨­å®šãªã—')}")
            logger.info(f"client.temperature: {getattr(client, 'temperature', 'è¨­å®šãªã—')}")
            logger.info(f"client.max_tokens: {getattr(client, 'max_tokens', 'è¨­å®šãªã—')}")
            logger.info(f"client.top_p: {getattr(client, 'top_p', 'è¨­å®šãªã—')}")
            logger.info(f"client.frequency_penalty: {getattr(client, 'frequency_penalty', 'è¨­å®šãªã—')}")
            logger.info(f"client.presence_penalty: {getattr(client, 'presence_penalty', 'è¨­å®šãªã—')}")

            return client
        except Exception as e:
            logger.error(f"AzureChatOpenAIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            raise

    # ãã‚Œä»¥å¤–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«ã¯å…ƒã®é–¢æ•°ã‚’ä½¿ç”¨
    logger.info(f"ãã®ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼({model_provider})ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™")
    client = original_init_chat_model(model=model, model_provider=model_provider, **kwargs)

    # structured_llmã®ä¸­èº«ã‚’ç¢ºèª
    logger.info("ã€structured_llmã®ä¸­èº«ã€‘:")
    logger.info(f"client.model: {getattr(client, 'model', 'è¨­å®šãªã—')}")
    logger.info(f"client.model_provider: {getattr(client, 'model_provider', 'è¨­å®šãªã—')}")
    logger.info(f"client.temperature: {getattr(client, 'temperature', 'è¨­å®šãªã—')}")
    logger.info(f"client.max_tokens: {getattr(client, 'max_tokens', 'è¨­å®šãªã—')}")
    logger.info(f"client.top_p: {getattr(client, 'top_p', 'è¨­å®šãªã—')}")
    logger.info(f"client.frequency_penalty: {getattr(client, 'frequency_penalty', 'è¨­å®šãªã—')}")
    logger.info(f"client.presence_penalty: {getattr(client, 'presence_penalty', 'è¨­å®šãªã—')}")

    return client

# ã‚ªãƒªã‚¸ãƒŠãƒ«é–¢æ•°ã‚’ç½®ãæ›ãˆ
lcm.init_chat_model = patched_init_chat_model
from open_deep_research.utils import deduplicate_and_format_sources
